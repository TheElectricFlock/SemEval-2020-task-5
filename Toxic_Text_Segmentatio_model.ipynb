{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdpSpxDEdTL1"
   },
   "source": [
    "# Toxic Text Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39679,
     "status": "ok",
     "timestamp": 1614114816560,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "SdQ_19h9w70K",
    "outputId": "f286025e-0f36-420a-c967-327415c4fd60"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import chars2vec \n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models, losses, callbacks, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PWm1tEArY6gG"
   },
   "outputs": [],
   "source": [
    "def f1(predictions, gold):\n",
    "    \"\"\"\n",
    "    Calculates F1 score(a.k.a. DICE)\n",
    "    \n",
    "    Args:\n",
    "        predictions: a list of predicted offsets\n",
    "        gold: a list of offsets serving as the ground truth\n",
    "        \n",
    "    Returns: \n",
    "        a float score between 0 and 1\n",
    "    \"\"\"\n",
    "    if len(gold) == 0:\n",
    "        return 1 if len(predictions) == 0 else 0\n",
    "    if len(predictions) == 0:\n",
    "        return 0\n",
    "    predictions_set = set(predictions)\n",
    "    gold_set = set(gold)\n",
    "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
    "    denom = len(predictions_set) + len(gold_set)\n",
    "    return float(nom)/float(denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PKEOlxEWy9dz"
   },
   "outputs": [],
   "source": [
    "def read_text_data(filename):\n",
    "    \"\"\"\n",
    "    Reads a csv file to extract text.\n",
    "    \n",
    "    Args:\n",
    "        filename: a string specifying the filename / path\n",
    "        \n",
    "    Returns:\n",
    "        A list of text sentences\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            data.append(row['text'])\n",
    "    csvfile.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to extract spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DI0YNeXszIAW"
   },
   "outputs": [],
   "source": [
    "def read_data_span(filename):\n",
    "    \"\"\"\n",
    "    Reads a csv file to extract the toxic spans.\n",
    "    \n",
    "    Args:\n",
    "        filename: a string specifying the filename / path\n",
    "        \n",
    "    Returns:\n",
    "        data: a list of strings of the toxic chars, \n",
    "        will look like '[1,2,3]' so it'll have to be split\n",
    "        \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            data.append(row['span'])\n",
    "    csvfile.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40577,
     "status": "ok",
     "timestamp": 1614114817551,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "SJyrue9gzNtg",
    "outputId": "b13d3049-756b-4544-c96a-0c0bbb673d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths equal: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = read_text_data('data/tsd_train_readable.csv')\n",
    "spans = read_data_span('data/tsd_train_readable.csv')\n",
    "texts.extend(read_text_data('data/tsd_trial_readable.csv'))\n",
    "spans.extend(read_data_span('data/tsd_trial_readable.csv'))\n",
    "\n",
    "\n",
    "processed_texts = []\n",
    "processed_spans = []\n",
    "print(f\"Lengths equal: {len(texts)==len(spans)}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the maximum comment size (in no. of chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sgC4OBpIzWLF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max size of sentence (in chars): 1000\n"
     ]
    }
   ],
   "source": [
    "max_size = 0\n",
    "for i in range(0, len(texts)-1):\n",
    "    if len(texts[i]) > max_size:\n",
    "        max_size = len(texts[i])\n",
    "print(f\"max size of sentence (in chars): {max_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the training data, after analysis the max sentence size is 1000 characters long, also removing empty strings and split the spans in to actual lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BPLticdKzTQ3"
   },
   "outputs": [],
   "source": [
    "c2v_model = chars2vec.load_model('eng_50')\n",
    "word_limit = 1024\n",
    "for i in range(0, len(texts)-1):\n",
    "    to_use = True\n",
    "    if len(texts[i]) > word_limit:\n",
    "        to_use = False\n",
    "    if texts[i] == \"\":\n",
    "        to_use = False\n",
    "    new_spans = [int(j) for j in spans[i][1:-1].split(\", \")]\n",
    "    if max(new_spans) > len(texts[i]) - 1:\n",
    "        to_use = False\n",
    "    if to_use:\n",
    "        if spans[i] != []:\n",
    "            full_span = [[0,0,1] for j in range(0, word_limit)]\n",
    "            for char_offset in new_spans:\n",
    "                full_span[char_offset] = [1,0,0]\n",
    "            for j in range(0, len(texts[i])-1):\n",
    "                if full_span[j][1] == 0 and full_span[j][2] == 1:\n",
    "                    full_span[j] = [0,1,0]\n",
    "        else:\n",
    "            full_span = [[1,0,0] for j in range(0, len(texts[i]))]           \n",
    "        processed_texts.append(texts[i])\n",
    "        processed_spans.append(full_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training and testing datasets with numpy zero arrays, this is to allow us to pad the end\n",
    "Of the toxic span with zeros as it is a fully convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7ErQHD8LzY1Z"
   },
   "outputs": [],
   "source": [
    "train_Y = np.zeros(shape=(len(processed_spans), 1024, 3))\n",
    "train_X = np.zeros(shape=(len(processed_texts), 1024, 50))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terrible Python best practise but you might wanna manually free up some memory. This is going to be a very large compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0nexzfqHyxC9"
   },
   "outputs": [],
   "source": [
    "del texts\n",
    "del spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Mlvr4eF9zfTl"
   },
   "outputs": [],
   "source": [
    "for x, string in enumerate(processed_texts):\n",
    "    for y, char in enumerate(string):\n",
    "            char_vect = c2v_model.vectorize_words([char])\n",
    "            train_X[x][y] = [word_vect for word_vect in char_vect[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c61dItQbzljv"
   },
   "outputs": [],
   "source": [
    "for x, label in enumerate(processed_spans):\n",
    "    for y, output in enumerate(label):\n",
    "        train_Y[x][y] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train_X and train_Y into training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9U660qMLzz8F"
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the shape of the train and val datasets, should be ([sample_size], 1000, 50) and ([sample_size], 1000, 3) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7102, 1024, 50)\n",
      "(7102, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a global variable would be out of scope for the callback object class manually create a HighScore class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighScore:\n",
    "    def __init__(self):\n",
    "        self.high_score = 0\n",
    "    def get_high_score(self):\n",
    "        return self.high_score\n",
    "    def set_high_score(self, new_score):\n",
    "        self.high_score = new_score\n",
    "high_score = HighScore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free up more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mbvc3atVucHH"
   },
   "outputs": [],
   "source": [
    "del processed_texts\n",
    "del processed_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tecSwLEfFTez"
   },
   "source": [
    "A prediction callback to act as a validation step, as the tensor is of a different shape to the F1 score of SemEval we must Convert it into it's proper form before checking the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Lh9DVvyz6NjV"
   },
   "outputs": [],
   "source": [
    "class PredictionCallback(callbacks.Callback):    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(val_X)\n",
    "        scores = []\n",
    "        for x, pred in enumerate(y_pred):\n",
    "            score = f1([j for j, i in enumerate(pred) if np.argmax(i) == 0], [j for j, i in enumerate(val_Y[x]) if np.argmax(i) == 0])\n",
    "            scores.append(score)\n",
    "        score = statistics.mean(scores)\n",
    "        if score > high_score.get_high_score():\n",
    "            high_score.set_high_score(score)\n",
    "            model.save(f\"UNet_checkpoint_sko\")\n",
    "        print(f\"F1 score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SegNet(is_deep=False, is_sko=False):\n",
    "    \n",
    "    if is_sko:\n",
    "        final_output_layer = 1\n",
    "    else:\n",
    "        final_output_layer = 9\n",
    "        \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape = train_X.shape[1:]))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv1D(filters=256, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    if is_deep:\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        model.add(layers.Conv1D(filters=512, kernel_size=9, strides=1, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.ELU())\n",
    "        model.add(layers.MaxPooling1D(strides=2))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        model.add(layers.UpSampling1D(size=2))\n",
    "        model.add(layers.Conv1D(filters=512, kernel_size=9, strides=1, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.ELU())\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1D(filters=256, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "\n",
    "    model.add(layers.Conv1D(filters=3, kernel_size=final_output_layer, strides=1, padding='same', activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def get_DeconvNet(is_deep=False, is_sko=False):\n",
    "    \n",
    "    if is_sko:\n",
    "        final_output_layer = 1\n",
    "    else:\n",
    "        final_output_layer = 9\n",
    "        \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape = train_X.shape[1:]))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv1D(filters=256, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    if is_deep:\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        model.add(layers.Conv1D(filters=512, kernel_size=9, strides=1, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.ELU())\n",
    "        model.add(layers.MaxPooling1D(strides=2))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        model.add(layers.UpSampling1D(size=2))\n",
    "        model.add(layers.Conv1DTranspose(filters=512, kernel_size=9, strides=1, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.ELU())\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1DTranspose(filters=256, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1DTranspose(filters=128, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1DTranspose(filters=64, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1DTranspose(filters=32, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "\n",
    "    model.add(layers.Conv1D(filters=3, kernel_size=final_output_layer, strides=1, padding='same', activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def get_UNet(is_deep=False, is_sko=False):\n",
    "    \n",
    "    if is_sko:\n",
    "        final_output_layer = 1\n",
    "    else:\n",
    "        final_output_layer = 9\n",
    "        \n",
    "    if is_deep:\n",
    "        encoder = [64, 128, 256, 512]\n",
    "        decoder = [512, 256, 128, 64, 32]\n",
    "    else:\n",
    "        encoder = [64, 128, 256]\n",
    "        decoder = [256, 128, 64, 32]\n",
    "        \n",
    "    # Entry conv block\n",
    "    inputs = layers.Input(shape = train_X.shape[1:])\n",
    "    x = layers.Conv1D(32, 3, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ELU()(x)\n",
    "    x = layers.MaxPooling1D(strides=2, padding=\"same\")(x)\n",
    "    \n",
    "    previous_block_activation = x\n",
    "    \n",
    "    for filters in encoder:\n",
    "        x = layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ELU()(x)\n",
    "        x = layers.MaxPooling1D(strides=2, padding=\"same\")(x)\n",
    "\n",
    "        residual = layers.Conv1D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "\n",
    "    for filters in decoder:\n",
    "        x = layers.Conv1DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ELU()(x)\n",
    "        x = layers.UpSampling1D(size=2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling1D(size=2)(previous_block_activation)\n",
    "        residual = layers.Conv1D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Classification layer\n",
    "    outputs = layers.Conv1D(3, final_output_layer, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model and view it's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 13759843,
     "status": "error",
     "timestamp": 1614130462871,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "H3YXwcUTzw6X",
    "outputId": "0e155ac1-fd98-431b-9b04-b9e983012148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 1024, 50)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1024, 32)     4832        input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 1024, 32)     128         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "elu_16 (ELU)                    (None, 1024, 32)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 512, 32)      0           elu_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 512, 64)      6208        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 512, 64)      256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "elu_17 (ELU)                    (None, 512, 64)      0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 256, 64)      0           elu_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 256, 64)      2112        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 256, 64)      0           max_pooling1d_10[0][0]           \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 256, 128)     24704       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 256, 128)     512         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "elu_18 (ELU)                    (None, 256, 128)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 128, 128)     0           elu_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 128, 128)     8320        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 128, 128)     0           max_pooling1d_11[0][0]           \n",
      "                                                                 conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 128, 256)     98560       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 128, 256)     1024        conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "elu_19 (ELU)                    (None, 128, 256)     0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 64, 256)      0           elu_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 64, 256)      33024       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 64, 256)      0           max_pooling1d_12[0][0]           \n",
      "                                                                 conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_transpose_4 (Conv1DTrans (None, 64, 256)      196864      add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 64, 256)      1024        conv1d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "elu_20 (ELU)                    (None, 64, 256)      0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_12 (UpSampling1D) (None, 128, 256)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_11 (UpSampling1D) (None, 128, 256)     0           elu_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 128, 256)     65792       up_sampling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 128, 256)     0           up_sampling1d_11[0][0]           \n",
      "                                                                 conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_transpose_5 (Conv1DTrans (None, 128, 128)     98432       add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 128, 128)     512         conv1d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "elu_21 (ELU)                    (None, 128, 128)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_14 (UpSampling1D) (None, 256, 256)     0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_13 (UpSampling1D) (None, 256, 128)     0           elu_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 256, 128)     32896       up_sampling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 256, 128)     0           up_sampling1d_13[0][0]           \n",
      "                                                                 conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_transpose_6 (Conv1DTrans (None, 256, 64)      24640       add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 256, 64)      256         conv1d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "elu_22 (ELU)                    (None, 256, 64)      0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_16 (UpSampling1D) (None, 512, 128)     0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_15 (UpSampling1D) (None, 512, 64)      0           elu_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 512, 64)      8256        up_sampling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 512, 64)      0           up_sampling1d_15[0][0]           \n",
      "                                                                 conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_transpose_7 (Conv1DTrans (None, 512, 32)      6176        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 512, 32)      128         conv1d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "elu_23 (ELU)                    (None, 512, 32)      0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_18 (UpSampling1D) (None, 1024, 64)     0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_17 (UpSampling1D) (None, 1024, 32)     0           elu_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 1024, 32)     2080        up_sampling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 1024, 32)     0           up_sampling1d_17[0][0]           \n",
      "                                                                 conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 1024, 3)      867         add_13[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 617,603\n",
      "Trainable params: 615,683\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_UNet(is_deep=False, is_sko=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "222/222 [==============================] - 31s 92ms/step - loss: 0.3081 - accuracy: 0.9482\n",
      "F1 score: 0\n",
      "Epoch 2/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0599 - accuracy: 0.9820\n",
      "F1 score: 0.11845222617867487\n",
      "Epoch 3/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0568 - accuracy: 0.9827\n",
      "F1 score: 0.18874367261018118\n",
      "Epoch 4/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0532 - accuracy: 0.9833\n",
      "F1 score: 0.21298432632411257\n",
      "Epoch 5/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0515 - accuracy: 0.9838\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.2510789061411174\n",
      "Epoch 6/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0494 - accuracy: 0.98460s - loss: 0.0494 - ac\n",
      "F1 score: 0.24648864400261503\n",
      "Epoch 7/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0492 - accuracy: 0.9843\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.35566884098632673\n",
      "Epoch 8/300\n",
      "222/222 [==============================] - 21s 92ms/step - loss: 0.0465 - accuracy: 0.9850\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.4097925933341966\n",
      "Epoch 9/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0468 - accuracy: 0.9842\n",
      "F1 score: 0.38665228081751796\n",
      "Epoch 10/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0447 - accuracy: 0.9846\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.4346802533879644\n",
      "Epoch 11/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0411 - accuracy: 0.9858\n",
      "F1 score: 0.31702883805475285\n",
      "Epoch 12/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0400 - accuracy: 0.98560s - loss: 0.0400 - accuracy: 0.98\n",
      "F1 score: 0.36173809968461157\n",
      "Epoch 13/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0368 - accuracy: 0.9866\n",
      "F1 score: 0.27176391858852816\n",
      "Epoch 14/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0366 - accuracy: 0.9865\n",
      "F1 score: 0.2604983909802964\n",
      "Epoch 15/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0384 - accuracy: 0.9858\n",
      "F1 score: 0.4315704031777787\n",
      "Epoch 16/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0297 - accuracy: 0.9892\n",
      "F1 score: 0.3704825131600288\n",
      "Epoch 17/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0279 - accuracy: 0.9898\n",
      "F1 score: 0.3678372822560849\n",
      "Epoch 18/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0276 - accuracy: 0.9899\n",
      "F1 score: 0.4153428657198045\n",
      "Epoch 19/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0234 - accuracy: 0.9915\n",
      "F1 score: 0.4210363093334325\n",
      "Epoch 20/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0221 - accuracy: 0.9920\n",
      "F1 score: 0.4138117055347053\n",
      "Epoch 21/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0217 - accuracy: 0.99200s - loss: 0.0217 - ac\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.44505732459765573\n",
      "Epoch 22/300\n",
      "222/222 [==============================] - 21s 96ms/step - loss: 0.0195 - accuracy: 0.9929\n",
      "F1 score: 0.4384279367676001\n",
      "Epoch 23/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0179 - accuracy: 0.9934\n",
      "F1 score: 0.4044805045234683\n",
      "Epoch 24/300\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 0.0174 - accuracy: 0.9936\n",
      "F1 score: 0.4359349367854246\n",
      "Epoch 25/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0162 - accuracy: 0.9940\n",
      "F1 score: 0.431061424978241\n",
      "Epoch 26/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0143 - accuracy: 0.9947\n",
      "F1 score: 0.40277516245124534\n",
      "Epoch 27/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0141 - accuracy: 0.9947\n",
      "F1 score: 0.4303682536557333\n",
      "Epoch 28/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0133 - accuracy: 0.9951\n",
      "F1 score: 0.41870449822018996\n",
      "Epoch 29/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0126 - accuracy: 0.9953\n",
      "F1 score: 0.41558290803517994\n",
      "Epoch 30/300\n",
      "222/222 [==============================] - 21s 95ms/step - loss: 0.0127 - accuracy: 0.9952\n",
      "F1 score: 0.4365269678029672\n",
      "Epoch 31/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0108 - accuracy: 0.9960\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.45362073124081576\n",
      "Epoch 32/300\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 0.0125 - accuracy: 0.9953\n",
      "F1 score: 0.3898449192611707\n",
      "Epoch 33/300\n",
      "222/222 [==============================] - 21s 95ms/step - loss: 0.0098 - accuracy: 0.99640s - loss: 0.0098 - accuracy: \n",
      "F1 score: 0.4011186961844063\n",
      "Epoch 34/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0110 - accuracy: 0.9958\n",
      "F1 score: 0.4455006339401367\n",
      "Epoch 35/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0083 - accuracy: 0.9969\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.4569950334748204\n",
      "Epoch 36/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0075 - accuracy: 0.9972\n",
      "F1 score: 0.4387783388754846\n",
      "Epoch 37/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0069 - accuracy: 0.9974\n",
      "F1 score: 0.4264345493964727\n",
      "Epoch 38/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0089 - accuracy: 0.9968\n",
      "F1 score: 0.41989645598764386\n",
      "Epoch 39/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0083 - accuracy: 0.9969\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.457089218875789\n",
      "Epoch 40/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0076 - accuracy: 0.9972\n",
      "F1 score: 0.4492670181725704\n",
      "Epoch 41/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0086 - accuracy: 0.9968\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.46178483776685797\n",
      "Epoch 42/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0074 - accuracy: 0.9972\n",
      "F1 score: 0.4470224832316033\n",
      "Epoch 43/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0059 - accuracy: 0.9978\n",
      "F1 score: 0.4070655642856498\n",
      "Epoch 44/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0063 - accuracy: 0.9977\n",
      "F1 score: 0.44987335107071963\n",
      "Epoch 45/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0075 - accuracy: 0.9973\n",
      "F1 score: 0.4379995952503865\n",
      "Epoch 46/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0118 - accuracy: 0.9956\n",
      "F1 score: 0.3990217413197772\n",
      "Epoch 47/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0069 - accuracy: 0.9975\n",
      "F1 score: 0.4324403129772372\n",
      "Epoch 48/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0041 - accuracy: 0.9985\n",
      "F1 score: 0.41399043250544215\n",
      "Epoch 49/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0035 - accuracy: 0.9988\n",
      "F1 score: 0.44069228140945277\n",
      "Epoch 50/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0030 - accuracy: 0.9989\n",
      "F1 score: 0.4489414074747996\n",
      "Epoch 51/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0027 - accuracy: 0.99901s -\n",
      "F1 score: 0.4542213170687222\n",
      "Epoch 52/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0107 - accuracy: 0.9961\n",
      "F1 score: 0.42371184871089645\n",
      "Epoch 53/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0090 - accuracy: 0.9967\n",
      "F1 score: 0.4364424790244537\n",
      "Epoch 54/300\n",
      "222/222 [==============================] - 21s 92ms/step - loss: 0.0045 - accuracy: 0.9983\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.46615447273527044\n",
      "Epoch 55/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0036 - accuracy: 0.9987\n",
      "F1 score: 0.4528200183271151\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0027 - accuracy: 0.9990\n",
      "F1 score: 0.4520137759636264\n",
      "Epoch 57/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0025 - accuracy: 0.9991\n",
      "F1 score: 0.4547932154759902\n",
      "Epoch 58/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0028 - accuracy: 0.9990\n",
      "F1 score: 0.40245312062657207\n",
      "Epoch 59/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0056 - accuracy: 0.9980\n",
      "F1 score: 0.3724416850271983\n",
      "Epoch 60/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0106 - accuracy: 0.9960\n",
      "F1 score: 0.3818988450198352\n",
      "Epoch 61/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0094 - accuracy: 0.9965\n",
      "F1 score: 0.43768835175352966\n",
      "Epoch 62/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0040 - accuracy: 0.9986\n",
      "F1 score: 0.43927287991996355\n",
      "Epoch 63/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0026 - accuracy: 0.9991\n",
      "F1 score: 0.4621761343900814\n",
      "Epoch 64/300\n",
      "222/222 [==============================] - 21s 92ms/step - loss: 0.0016 - accuracy: 0.9995\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.46799475311137073\n",
      "Epoch 65/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0016 - accuracy: 0.9995\n",
      "F1 score: 0.46767304893959755\n",
      "Epoch 66/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0025 - accuracy: 0.9991\n",
      "F1 score: 0.4461512367197891\n",
      "Epoch 67/300\n",
      "222/222 [==============================] - 20s 88ms/step - loss: 0.0049 - accuracy: 0.99820s - loss: 0.0048 \n",
      "F1 score: 0.45182311740287884\n",
      "Epoch 68/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0055 - accuracy: 0.9980\n",
      "F1 score: 0.3998217855101773\n",
      "Epoch 69/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0071 - accuracy: 0.9973\n",
      "F1 score: 0.4364518826038341\n",
      "Epoch 70/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0039 - accuracy: 0.9986\n",
      "F1 score: 0.4562372284910805\n",
      "Epoch 71/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0023 - accuracy: 0.9992\n",
      "F1 score: 0.45211386350881\n",
      "Epoch 72/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0014 - accuracy: 0.9995\n",
      "F1 score: 0.42109988718884844\n",
      "Epoch 73/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0030 - accuracy: 0.9990\n",
      "F1 score: 0.46119223140784843\n",
      "Epoch 74/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0082 - accuracy: 0.99701s - los\n",
      "F1 score: 0.4395780358134763\n",
      "Epoch 75/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0056 - accuracy: 0.9979\n",
      "F1 score: 0.4636728843890839\n",
      "Epoch 76/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0020 - accuracy: 0.99930s - loss: 0.0\n",
      "F1 score: 0.4459003848305082\n",
      "Epoch 77/300\n",
      "222/222 [==============================] - 20s 88ms/step - loss: 0.0019 - accuracy: 0.9994\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.4711125635363026\n",
      "Epoch 78/300\n",
      "222/222 [==============================] - 21s 92ms/step - loss: 9.2867e-04 - accuracy: 0.9997\n",
      "F1 score: 0.4124137372050124\n",
      "Epoch 79/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "F1 score: 0.43798710658112033\n",
      "Epoch 80/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0100 - accuracy: 0.9965\n",
      "F1 score: 0.4421750047378798\n",
      "Epoch 81/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0041 - accuracy: 0.9985\n",
      "F1 score: 0.4571869446108583\n",
      "Epoch 82/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0020 - accuracy: 0.99930s - loss: 0.0020 \n",
      "F1 score: 0.44423049118646546\n",
      "Epoch 83/300\n",
      "222/222 [==============================] - 20s 88ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "F1 score: 0.4491079890318783\n",
      "Epoch 84/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0043 - accuracy: 0.9987\n",
      "F1 score: 0.45707048764616\n",
      "Epoch 85/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0045 - accuracy: 0.99841s - los\n",
      "F1 score: 0.4479582800960096\n",
      "Epoch 86/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0016 - accuracy: 0.9994\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.4712990716171665\n",
      "Epoch 87/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "F1 score: 0.4702299789447408\n",
      "Epoch 88/300\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 7.0334e-04 - accuracy: 0.9998\n",
      "F1 score: 0.4635200950325452\n",
      "Epoch 89/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 7.2035e-04 - accuracy: 0.9998\n",
      "F1 score: 0.4280824249549032\n",
      "Epoch 90/300\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 0.0020 - accuracy: 0.9993\n",
      "F1 score: 0.4284839166104222\n",
      "Epoch 91/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0113 - accuracy: 0.9959\n",
      "F1 score: 0.4630581315066446\n",
      "Epoch 92/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0063 - accuracy: 0.9977\n",
      "F1 score: 0.45980854334645926\n",
      "Epoch 93/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0018 - accuracy: 0.9994\n",
      "F1 score: 0.46381735132548174\n",
      "Epoch 94/300\n",
      "222/222 [==============================] - 20s 88ms/step - loss: 7.2557e-04 - accuracy: 0.9998\n",
      "F1 score: 0.45052095239159673\n",
      "Epoch 95/300\n",
      "222/222 [==============================] - 20s 88ms/step - loss: 8.9687e-04 - accuracy: 0.99970s - loss: 8.9540e-04 - accuracy: 0.\n",
      "F1 score: 0.45387848949469645\n",
      "Epoch 96/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "F1 score: 0.4612380715720663\n",
      "Epoch 97/300\n",
      "222/222 [==============================] - 21s 94ms/step - loss: 0.0016 - accuracy: 0.9995\n",
      "F1 score: 0.4552099364297748\n",
      "Epoch 98/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0036 - accuracy: 0.9987\n",
      "F1 score: 0.3902229491727238\n",
      "Epoch 99/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0081 - accuracy: 0.9970\n",
      "F1 score: 0.44085435329197475\n",
      "Epoch 100/300\n",
      "222/222 [==============================] - 21s 97ms/step - loss: 0.0017 - accuracy: 0.9994\n",
      "F1 score: 0.46424521978912237\n",
      "Epoch 101/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 8.6840e-04 - accuracy: 0.9997\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.4802660840695284\n",
      "Epoch 102/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 5.6133e-04 - accuracy: 0.9998\n",
      "INFO:tensorflow:Assets written to: UNet_checkpoint_sko\\assets\n",
      "F1 score: 0.48213407947758313\n",
      "Epoch 103/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 7.9252e-04 - accuracy: 0.9998\n",
      "F1 score: 0.4545109423996191\n",
      "Epoch 104/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 5.2350e-04 - accuracy: 0.9998\n",
      "F1 score: 0.4132926099635322\n",
      "Epoch 105/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0028 - accuracy: 0.99900s - loss: 0.0027 - ac\n",
      "F1 score: 0.4308541885696569\n",
      "Epoch 106/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0067 - accuracy: 0.9975\n",
      "F1 score: 0.44311994674271765\n",
      "Epoch 107/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0054 - accuracy: 0.9980\n",
      "F1 score: 0.3983067124432845\n",
      "Epoch 108/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0039 - accuracy: 0.9986\n",
      "F1 score: 0.44819357967361134\n",
      "Epoch 109/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0011 - accuracy: 0.9996\n",
      "F1 score: 0.46797034687733235\n",
      "Epoch 110/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0013 - accuracy: 0.9996\n",
      "F1 score: 0.4596947171022209\n",
      "Epoch 111/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 4.1881e-04 - accuracy: 0.9999\n",
      "F1 score: 0.45769973754707166\n",
      "Epoch 112/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0065 - accuracy: 0.9977\n",
      "F1 score: 0.4494993566096884\n",
      "Epoch 113/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0027 - accuracy: 0.9991\n",
      "F1 score: 0.47139467354039716\n",
      "Epoch 114/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0012 - accuracy: 0.9996\n",
      "F1 score: 0.46064540075091714\n",
      "Epoch 115/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 7.2579e-04 - accuracy: 0.9998\n",
      "F1 score: 0.42890063598371886\n",
      "Epoch 116/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 8.0770e-04 - accuracy: 0.9997\n",
      "F1 score: 0.4789163895270474\n",
      "Epoch 117/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 9.2308e-04 - accuracy: 0.9997\n",
      "F1 score: 0.46001576309377656\n",
      "Epoch 118/300\n",
      "222/222 [==============================] - 21s 92ms/step - loss: 0.0033 - accuracy: 0.9989\n",
      "F1 score: 0.4423820446999964\n",
      "Epoch 119/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0060 - accuracy: 0.9979\n",
      "F1 score: 0.45265168097951053\n",
      "Epoch 120/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0017 - accuracy: 0.9994\n",
      "F1 score: 0.46558216009181985\n",
      "Epoch 121/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 7.2656e-04 - accuracy: 0.9998\n",
      "F1 score: 0.46929668627054594\n",
      "Epoch 122/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 4.5799e-04 - accuracy: 0.9999\n",
      "F1 score: 0.4640161864922344\n",
      "Epoch 123/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 3.6496e-04 - accuracy: 0.99990s - loss: 3.6425e-04 - accuracy: \n",
      "F1 score: 0.46461350905876453\n",
      "Epoch 124/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 9.0177e-04 - accuracy: 0.9997\n",
      "F1 score: 0.4648785704623701\n",
      "Epoch 125/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 9.1713e-04 - accuracy: 0.99972s - loss: 7 - ETA: 0s - loss: 8.5715e\n",
      "F1 score: 0.4184972521788915\n",
      "Epoch 126/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0079 - accuracy: 0.9972\n",
      "F1 score: 0.43602125691011845\n",
      "Epoch 127/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0031 - accuracy: 0.9989\n",
      "F1 score: 0.4448226495009217\n",
      "Epoch 128/300\n",
      "222/222 [==============================] - 20s 92ms/step - loss: 0.0012 - accuracy: 0.99960s - loss: 0.0012 - ac\n",
      "F1 score: 0.46276093540508517\n",
      "Epoch 129/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 5.8064e-04 - accuracy: 0.9998\n",
      "F1 score: 0.48006854027312745\n",
      "Epoch 130/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 5.7444e-04 - accuracy: 0.9998\n",
      "F1 score: 0.4766537241900903\n",
      "Epoch 131/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 5.8720e-04 - accuracy: 0.9998\n",
      "F1 score: 0.4724422426156745\n",
      "Epoch 132/300\n",
      "222/222 [==============================] - 21s 93ms/step - loss: 0.0011 - accuracy: 0.9996\n",
      "F1 score: 0.44333421405273\n",
      "Epoch 133/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 0.0076 - accuracy: 0.9972\n",
      "F1 score: 0.3997320897447848\n",
      "Epoch 134/300\n",
      "222/222 [==============================] - 20s 89ms/step - loss: 0.0046 - accuracy: 0.9983\n",
      "F1 score: 0.46250341101341363\n",
      "Epoch 135/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 0.0015 - accuracy: 0.9995\n",
      "F1 score: 0.4682332683215864\n",
      "Epoch 136/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 5.6450e-04 - accuracy: 0.9998\n",
      "F1 score: 0.4751480583362908\n",
      "Epoch 137/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 3.4578e-04 - accuracy: 0.9999\n",
      "F1 score: 0.4316931381052365\n",
      "Epoch 138/300\n",
      "222/222 [==============================] - 20s 90ms/step - loss: 4.0613e-04 - accuracy: 0.99991s - loss: 4.0047e-04 - accura - ETA: 1s - loss: 4\n",
      "F1 score: 0.46990026490610337\n",
      "Epoch 139/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 3.8040e-04 - accuracy: 0.99990s - loss: 3.7948e-04 - accuracy: 0.\n",
      "F1 score: 0.4819138055034296\n",
      "Epoch 140/300\n",
      "222/222 [==============================] - 20s 91ms/step - loss: 5.0841e-04 - accuracy: 0.9999\n",
      "F1 score: 0.4397861661487659\n",
      "Epoch 141/300\n",
      "114/222 [==============>...............] - ETA: 10s - loss: 0.0081 - accuracy: 0.9972"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, train_Y, epochs=300, batch_size=32, callbacks=[PredictionCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model and check final validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rACAu8nCZ01L"
   },
   "outputs": [],
   "source": [
    "model.save(f\"UNet_model_300_epochs_sko\")\n",
    "scores = []\n",
    "pred_Y = model.predict(val_X)\n",
    "\n",
    "for x, pred in enumerate(pred_Y):\n",
    "    y_pred_f1_compatible = [j for j, i in enumerate(pred) if np.argmax(i) == 0]\n",
    "    y_true_f1_compatible = [j for j, i in enumerate(val_Y[x]) if np.argmax(i) == 0]\n",
    "    score = f1(y_pred_f1_compatible, y_true_f1_compatible)\n",
    "    scores.append(score)\n",
    "\n",
    "print('avg F1 %g' % statistics.mean(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually view some predictions to check validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SgYIaccv1Mj"
   },
   "outputs": [],
   "source": [
    "for x, pred in enumerate(pred_Y):\n",
    "    score = f1([j for j, i in enumerate(pred) if np.argmax(i) == 0], [j for j, i in enumerate(val_Y[x]) if np.argmax(i) == 0])\n",
    "    print(f\"F1 score: {score}\")\n",
    "    print(f\"Predicted categorical: {[np.argmax(i) for i in pred]}\")\n",
    "    print(f\"Predicted span: {[j for j, i in enumerate(pred) if np.argmax(i) == 0]}\")\n",
    "    print(f\"Ground truth span: {[j for j, i in enumerate(val_Y[x]) if np.argmax(i) == 0]}\" + \"\\n\")\n",
    "    if x == 100:\n",
    "          break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP79GJAw0365QrEWLKMJifk",
   "collapsed_sections": [],
   "name": "Toxic_Text_Segmentatio_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
