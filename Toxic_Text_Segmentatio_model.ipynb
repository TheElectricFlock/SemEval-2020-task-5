{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdpSpxDEdTL1"
   },
   "source": [
    "# Toxic Text Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39679,
     "status": "ok",
     "timestamp": 1614114816560,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "SdQ_19h9w70K",
    "outputId": "f286025e-0f36-420a-c967-327415c4fd60"
   },
   "outputs": [],
   "source": [
    "!pip install chars2vec\n",
    "import csv\n",
    "import chars2vec \n",
    "import re\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import datasets, layers, models, losses, callbacks, Model\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWm1tEArY6gG"
   },
   "outputs": [],
   "source": [
    "def f1(predictions, gold):\n",
    "    \"\"\"\n",
    "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
    "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
    "    :param predictions: a list of predicted offsets\n",
    "    :param gold: a list of offsets serving as the ground truth\n",
    "    :return: a score between 0 and 1\n",
    "    \"\"\"\n",
    "    if len(gold) == 0:\n",
    "        return 1 if len(predictions) == 0 else 0\n",
    "    if len(predictions) == 0:\n",
    "        return 0\n",
    "    predictions_set = set(predictions)\n",
    "    gold_set = set(gold)\n",
    "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
    "    denom = len(predictions_set) + len(gold_set)\n",
    "    return float(nom)/float(denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKEOlxEWy9dz"
   },
   "outputs": [],
   "source": [
    "def read_text_data(filename):\n",
    "    \"\"\"Reads csv file with python, text.\"\"\"\n",
    "    data = []\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            data.append(row['text'])\n",
    "    csvfile.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DI0YNeXszIAW"
   },
   "outputs": [],
   "source": [
    "def read_data_span(filename):\n",
    "    \"\"\"Reads csv file with python, span list.\"\"\"\n",
    "    data = []\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            data.append(row['span'])\n",
    "    csvfile.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlbJqSZdzKu7"
   },
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40577,
     "status": "ok",
     "timestamp": 1614114817551,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "SJyrue9gzNtg",
    "outputId": "b13d3049-756b-4544-c96a-0c0bbb673d74"
   },
   "outputs": [],
   "source": [
    "texts = read_text_data('gdrive/My Drive/Colab Notebooks/Data/tsd_train_readable.csv')\n",
    "spans = read_data_span('gdrive/My Drive/Colab Notebooks/Data/tsd_train_readable.csv')\n",
    "texts.extend(read_text_data('gdrive/My Drive/Colab Notebooks/Data/tsd_trial_readable.csv'))\n",
    "spans.extend(read_data_span('gdrive/My Drive/Colab Notebooks/Data/tsd_trial_readable.csv'))\n",
    "\n",
    "\n",
    "processed_texts = []\n",
    "processed_spans = []\n",
    "print(f\"Lengths equal: {len(texts)==len(spans)}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPLticdKzTQ3"
   },
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "c2v_model = chars2vec.load_model('eng_50')\n",
    "word_limit = 1000\n",
    "for i in range(0, len(texts)-1):\n",
    "    to_use = True\n",
    "    if len(texts[i]) > word_limit:\n",
    "        to_use = False\n",
    "    if texts[i] == \"\":\n",
    "        to_use = False\n",
    "    new_spans = [int(j) for j in spans[i][1:-1].split(\", \")]\n",
    "    if max(new_spans) > len(texts[i]) - 1:\n",
    "        to_use = False\n",
    "    if to_use:\n",
    "        if spans[i] != []:\n",
    "            full_span = [[0,0,1] for j in range(0, word_limit)]\n",
    "            for char_offset in new_spans:\n",
    "                full_span[char_offset] = [1,0,0]\n",
    "            for j in range(0, len(texts[i])-1):\n",
    "                if full_span[j][1] == 0 and full_span[j][2] == 1:\n",
    "                    full_span[j] = [0,1,0]\n",
    "        else:\n",
    "            full_span = [[1,0,0] for j in range(0, len(texts[i]))]           \n",
    "        processed_texts.append(texts[i])\n",
    "        processed_spans.append(full_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgC4OBpIzWLF"
   },
   "outputs": [],
   "source": [
    "# Get the maximim comment size (in no. of chars)\n",
    "max_size = 0\n",
    "for i in range(0, len(processed_texts)-1):\n",
    "    if len(processed_texts[i]) > max_size:\n",
    "        max_size = len(processed_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49682,
     "status": "ok",
     "timestamp": 1614114826714,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "mSXC7j5d9yk1",
    "outputId": "273dea95-6717-49db-8189-d32f9df21791"
   },
   "outputs": [],
   "source": [
    "max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ErQHD8LzY1Z"
   },
   "outputs": [],
   "source": [
    "# Define the training arrays\n",
    "train_Y = np.zeros(shape=(len(processed_spans), max_size, 3))\n",
    "train_X = np.zeros(shape=(len(processed_texts), max_size, 50))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nexzfqHyxC9"
   },
   "outputs": [],
   "source": [
    "del texts\n",
    "del spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mlvr4eF9zfTl"
   },
   "outputs": [],
   "source": [
    "# Build Train_X\n",
    "for x, string in enumerate(processed_texts):\n",
    "    for y, char in enumerate(string):\n",
    "            char_vect = c2v_model.vectorize_words([char])\n",
    "            train_X[x][y] = [word_vect for word_vect in char_vect[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c61dItQbzljv"
   },
   "outputs": [],
   "source": [
    "# Build train_Y\n",
    "for x, label in enumerate(processed_spans):\n",
    "    for y, output in enumerate(label):\n",
    "        train_Y[x][y] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U660qMLzz8F"
   },
   "outputs": [],
   "source": [
    "# Build test_X, Test_Y\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(train_X, train_Y, test_size=0.1, random_state=42)\n",
    "class High_Score:\n",
    "    def __init__(self):\n",
    "        self.high_score = 0\n",
    "    def get_high_score(self):\n",
    "        return self.high_score\n",
    "    def set_high_score(self, new_score):\n",
    "        self.high_score = new_score\n",
    "high_score = High_Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116636,
     "status": "ok",
     "timestamp": 1614114893782,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "Wmw_XiUITJ2d",
    "outputId": "cace2575-b93b-4beb-f4f6-0f4efa3ba5a8"
   },
   "outputs": [],
   "source": [
    "print(train_Y.shape)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbvc3atVucHH"
   },
   "outputs": [],
   "source": [
    "del processed_texts\n",
    "del processed_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tecSwLEfFTez"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lh9DVvyz6NjV"
   },
   "outputs": [],
   "source": [
    "class PredictionCallback(callbacks.Callback):    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(test_X)\n",
    "        scores = []\n",
    "        for x, pred in enumerate(y_pred):\n",
    "            score = f1([j for j, i in enumerate(pred) if np.argmax(i) == 0], [j for j, i in enumerate(test_Y[x]) if np.argmax(i) == 0])\n",
    "            scores.append(score)\n",
    "        score = statistics.mean(scores)\n",
    "        if score > high_score.get_high_score():\n",
    "            high_score.set_high_score(score)\n",
    "            model.save(f\"{root_path}model_autoencoder_LSTM_checkpoint\")\n",
    "        print(f\"F1 score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XE8QY_opI8HD"
   },
   "outputs": [],
   "source": [
    "# create architecture\n",
    "#model = models.Sequential()\n",
    "# vocabulary size — number of unique words in data\n",
    "# length of vector with which each word is represented\n",
    "#model.add(layers.Input(shape = train_X.shape[1:]))\n",
    "# add an LSTM layer which contains 64 LSTM cells\n",
    "# True — return whole sequence; False — return single output of the end of the sequence\n",
    "#model.add(layers.Dropout(0.3))\n",
    "#model.add(layers.GRU(128, return_sequences=True))\n",
    "#model.add(layers.RepeatVector(1000))\n",
    "#model.add(layers.GRU(256, return_sequences=True))\n",
    "#model.add(layers.Dropout(0.3))\n",
    "#model.add(layers.TimeDistributed(layers.Dense(3, activation='softmax')))\n",
    "#compile model\n",
    "#model.compile(loss      =  'categorical_crossentropy',\n",
    "#                  optimizer =  'adam',\n",
    "#                  metrics   =  ['acc'])\n",
    "# check summary of the model\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 13759843,
     "status": "error",
     "timestamp": 1614130462871,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "H3YXwcUTzw6X",
    "outputId": "0e155ac1-fd98-431b-9b04-b9e983012148"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Input(shape = train_X.shape[1:]))\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.MaxPooling1D(strides=2))\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.MaxPooling1D(strides=2))\n",
    "model.add(layers.Conv1D(filters=128, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.MaxPooling1D(strides=2))\n",
    "model.add(layers.Conv1D(filters=256, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Bidirectional(layers.GRU(units=128, return_sequences=True)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Bidirectional(layers.GRU(units=128, return_sequences=True)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Conv1D(filters=256, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Conv1DTranspose(filters=128, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.UpSampling1D())\n",
    "model.add(layers.Conv1DTranspose(filters=64, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.UpSampling1D())\n",
    "model.add(layers.Conv1DTranspose(filters=32, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.UpSampling1D())\n",
    "model.add(layers.Conv1D(filters=3, kernel_size=9, strides=1, padding='same', activation='softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "history = model.fit(train_X, train_Y, epochs=300, batch_size=32, callbacks=[PredictionCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rACAu8nCZ01L"
   },
   "outputs": [],
   "source": [
    "model.save(f\"{root_path}model_autoencoder_LSTM\")\n",
    "scores = []\n",
    "pred_Y = model.predict(test_X)\n",
    "\n",
    "for x, pred in enumerate(pred_Y):\n",
    "    y_pred_f1_compatible = [j for j, i in enumerate(pred) if np.argmax(i) == 0]\n",
    "    y_true_f1_compatible = [j for j, i in enumerate(test_Y[x]) if np.argmax(i) == 0]\n",
    "    if test_Y[x] == []:\n",
    "        y_pred_f1_compatible = []\n",
    "    score = f1(y_pred_f1_compatible, y_true_f1_compatible)\n",
    "    scores.append(score)\n",
    "\n",
    "print('avg F1 %g' % statistics.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SgYIaccv1Mj"
   },
   "outputs": [],
   "source": [
    "for x, pred in enumerate(pred_Y):\n",
    "    score = f1([j for j, i in enumerate(pred) if np.argmax(i) == 0], [j for j, i in enumerate(test_Y[x]) if np.argmax(i) == 0])\n",
    "    print(f\"F1 score: {score}\")\n",
    "    print(f\"Predicted span one_hot: {[np.argmax(i) for i in pred]}\")\n",
    "    print(f\"Predicted span: {[j for j, i in enumerate(pred) if np.argmax(i) == 0]}\")\n",
    "    print(f\"Ground truth span: {[j for j, i in enumerate(test_Y[x]) if np.argmax(i) == 0]}\" + \"\\n\")\n",
    "    if x == 100:\n",
    "          break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP79GJAw0365QrEWLKMJifk",
   "collapsed_sections": [],
   "name": "Toxic_Text_Segmentatio_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
