{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdpSpxDEdTL1"
   },
   "source": [
    "# Toxic Text Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39679,
     "status": "ok",
     "timestamp": 1614114816560,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "SdQ_19h9w70K",
    "outputId": "f286025e-0f36-420a-c967-327415c4fd60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chars2vec in c:\\users\\james\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.1.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\james\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install chars2vec\n",
    "import csv\n",
    "import chars2vec \n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models, losses, callbacks, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PWm1tEArY6gG"
   },
   "outputs": [],
   "source": [
    "def f1(predictions, gold):\n",
    "    \"\"\"\n",
    "    Calculates F1 score(a.k.a. DICE)\n",
    "    \n",
    "    Args:\n",
    "        predictions: a list of predicted offsets\n",
    "        gold: a list of offsets serving as the ground truth\n",
    "        \n",
    "    Returns: \n",
    "        a float score between 0 and 1\n",
    "    \"\"\"\n",
    "    if len(gold) == 0:\n",
    "        return 1 if len(predictions) == 0 else 0\n",
    "    if len(predictions) == 0:\n",
    "        return 0\n",
    "    predictions_set = set(predictions)\n",
    "    gold_set = set(gold)\n",
    "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
    "    denom = len(predictions_set) + len(gold_set)\n",
    "    return float(nom)/float(denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PKEOlxEWy9dz"
   },
   "outputs": [],
   "source": [
    "def read_text_data(filename):\n",
    "    \"\"\"\n",
    "    Reads a csv file to extract text.\n",
    "    \n",
    "    Args:\n",
    "        filename: a string specifying the filename / path\n",
    "        \n",
    "    Returns:\n",
    "        A list of text sentences\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            data.append(row['text'])\n",
    "    csvfile.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to extract spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DI0YNeXszIAW"
   },
   "outputs": [],
   "source": [
    "def read_data_span(filename):\n",
    "    \"\"\"\n",
    "    Reads a csv file to extract the toxic spans.\n",
    "    \n",
    "    Args:\n",
    "        filename: a string specifying the filename / path\n",
    "        \n",
    "    Returns:\n",
    "        data: a list of strings of the toxic chars, \n",
    "        will look like '[1,2,3]' so it'll have to be split\n",
    "        \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            data.append(row['span'])\n",
    "    csvfile.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40577,
     "status": "ok",
     "timestamp": 1614114817551,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "SJyrue9gzNtg",
    "outputId": "b13d3049-756b-4544-c96a-0c0bbb673d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths equal: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = read_text_data('../data/tsd_train_readable.csv')\n",
    "spans = read_data_span('../data/tsd_train_readable.csv')\n",
    "texts.extend(read_text_data('../data/tsd_trial_readable.csv'))\n",
    "spans.extend(read_data_span('../data/tsd_trial_readable.csv'))\n",
    "\n",
    "\n",
    "processed_texts = []\n",
    "processed_spans = []\n",
    "print(f\"Lengths equal: {len(texts)==len(spans)}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the training data, after analysis the max sentence size is 1000 characters long, also removing empty strings and split the spans in to actual lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BPLticdKzTQ3"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras' has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5fd71380b489>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mc2v_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchars2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'eng_50'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mword_limit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mto_use\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mword_limit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\chars2vec\\model.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0memb_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_to_ix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     \u001b[0mc2v_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChars2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_to_ix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m     \u001b[0mc2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_model\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/weights.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[0mc2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\chars2vec\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, emb_dim, char_to_ix)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mlstm_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras' has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "c2v_model = chars2vec.load_model('eng_50')\n",
    "word_limit = 1000\n",
    "for i in range(0, len(texts)-1):\n",
    "    to_use = True\n",
    "    if len(texts[i]) > word_limit:\n",
    "        to_use = False\n",
    "    if texts[i] == \"\":\n",
    "        to_use = False\n",
    "    new_spans = [int(j) for j in spans[i][1:-1].split(\", \")]\n",
    "    if max(new_spans) > len(texts[i]) - 1:\n",
    "        to_use = False\n",
    "    if to_use:\n",
    "        if spans[i] != []:\n",
    "            full_span = [[0,0,1] for j in range(0, word_limit)]\n",
    "            for char_offset in new_spans:\n",
    "                full_span[char_offset] = [1,0,0]\n",
    "            for j in range(0, len(texts[i])-1):\n",
    "                if full_span[j][1] == 0 and full_span[j][2] == 1:\n",
    "                    full_span[j] = [0,1,0]\n",
    "        else:\n",
    "            full_span = [[1,0,0] for j in range(0, len(texts[i]))]           \n",
    "        processed_texts.append(texts[i])\n",
    "        processed_spans.append(full_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the maximim comment size (in no. of chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgC4OBpIzWLF"
   },
   "outputs": [],
   "source": [
    "\n",
    "max_size = 0\n",
    "for i in range(0, len(processed_texts)-1):\n",
    "    if len(processed_texts[i]) > max_size:\n",
    "        max_size = len(processed_texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training and testing datasets with numpy zero arrays, this is to allow us to pad the end\n",
    "Of the toxic span with zeros as it is a fully convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ErQHD8LzY1Z"
   },
   "outputs": [],
   "source": [
    "train_Y = np.zeros(shape=(len(processed_spans), max_size, 3))\n",
    "train_X = np.zeros(shape=(len(processed_texts), max_size, 50))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terrible Python best practise but you might wanna manually free up some memory. This is going to be a very large compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nexzfqHyxC9"
   },
   "outputs": [],
   "source": [
    "del texts\n",
    "del spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mlvr4eF9zfTl"
   },
   "outputs": [],
   "source": [
    "for x, string in enumerate(processed_texts):\n",
    "    for y, char in enumerate(string):\n",
    "            char_vect = c2v_model.vectorize_words([char])\n",
    "            train_X[x][y] = [word_vect for word_vect in char_vect[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c61dItQbzljv"
   },
   "outputs": [],
   "source": [
    "for x, label in enumerate(processed_spans):\n",
    "    for y, output in enumerate(label):\n",
    "        train_Y[x][y] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train_X and train_Y into training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U660qMLzz8F"
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the shape of the train and val datasets, should be ([sample_size], 1000, 50) and ([sample_size], 1000, 3) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a global variable would be out of scope for the callback object class manually create a HighScore class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighScore:\n",
    "    def __init__(self):\n",
    "        self.high_score = 0\n",
    "    def get_high_score(self):\n",
    "        return self.high_score\n",
    "    def set_high_score(self, new_score):\n",
    "        self.high_score = new_score\n",
    "high_score = HighScore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free up more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbvc3atVucHH"
   },
   "outputs": [],
   "source": [
    "del processed_texts\n",
    "del processed_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tecSwLEfFTez"
   },
   "source": [
    "A prediction callback to act as a validation step, as the tensor is of a different shape to the F1 score of SemEval we must Convert it into it's proper form before checking the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lh9DVvyz6NjV"
   },
   "outputs": [],
   "source": [
    "class PredictionCallback(callbacks.Callback):    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(test_X)\n",
    "        scores = []\n",
    "        for x, pred in enumerate(y_pred):\n",
    "            score = f1([j for j, i in enumerate(pred) if np.argmax(i) == 0], [j for j, i in enumerate(test_Y[x]) if np.argmax(i) == 0])\n",
    "            scores.append(score)\n",
    "        score = statistics.mean(scores)\n",
    "        if score > high_score.get_high_score():\n",
    "            high_score.set_high_score(score)\n",
    "            model.save(f\"{root_path}model_autoencoder_LSTM_checkpoint\")\n",
    "        print(f\"F1 score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write a \"get_model\" function\n",
    "# def get_model(type):\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 13759843,
     "status": "error",
     "timestamp": 1614130462871,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "H3YXwcUTzw6X",
    "outputId": "0e155ac1-fd98-431b-9b04-b9e983012148"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Input(shape = train_X.shape[1:]))\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.MaxPooling1D(strides=2))\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.MaxPooling1D(strides=2))\n",
    "model.add(layers.Conv1D(filters=128, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.MaxPooling1D(strides=2))\n",
    "model.add(layers.Conv1D(filters=256, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Bidirectional(layers.GRU(units=128, return_sequences=True)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Bidirectional(layers.GRU(units=128, return_sequences=True)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Conv1D(filters=256, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Conv1DTranspose(filters=128, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.UpSampling1D())\n",
    "model.add(layers.Conv1DTranspose(filters=64, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.UpSampling1D())\n",
    "model.add(layers.Conv1DTranspose(filters=32, kernel_size=9, strides=1, padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.ELU())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.UpSampling1D())\n",
    "model.add(layers.Conv1D(filters=3, kernel_size=9, strides=1, padding='same', activation='softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_Y, epochs=300, batch_size=32, callbacks=[PredictionCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model and check final validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rACAu8nCZ01L"
   },
   "outputs": [],
   "source": [
    "model.save(f\"{root_path}model_autoencoder_LSTM\")\n",
    "scores = []\n",
    "pred_Y = model.predict(val_X)\n",
    "\n",
    "for x, pred in enumerate(pred_Y):\n",
    "    y_pred_f1_compatible = [j for j, i in enumerate(pred) if np.argmax(i) == 0]\n",
    "    y_true_f1_compatible = [j for j, i in enumerate(val_Y[x]) if np.argmax(i) == 0]\n",
    "    if val_Y[x] == []:\n",
    "        y_pred_f1_compatible = []\n",
    "    score = f1(y_pred_f1_compatible, y_true_f1_compatible)\n",
    "    scores.append(score)\n",
    "\n",
    "print('avg F1 %g' % statistics.mean(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually view some predictions to check validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SgYIaccv1Mj"
   },
   "outputs": [],
   "source": [
    "for x, pred in enumerate(pred_Y):\n",
    "    score = f1([j for j, i in enumerate(pred) if np.argmax(i) == 0], [j for j, i in enumerate(val_Y[x]) if np.argmax(i) == 0])\n",
    "    print(f\"F1 score: {score}\")\n",
    "    print(f\"Predicted span one_hot: {[np.argmax(i) for i in pred]}\")\n",
    "    print(f\"Predicted span: {[j for j, i in enumerate(pred) if np.argmax(i) == 0]}\")\n",
    "    print(f\"Ground truth span: {[j for j, i in enumerate(val_Y[x]) if np.argmax(i) == 0]}\" + \"\\n\")\n",
    "    if x == 100:\n",
    "          break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP79GJAw0365QrEWLKMJifk",
   "collapsed_sections": [],
   "name": "Toxic_Text_Segmentatio_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
