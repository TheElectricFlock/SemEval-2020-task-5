{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdpSpxDEdTL1"
   },
   "source": [
    "# Toxic Text Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39679,
     "status": "ok",
     "timestamp": 1614114816560,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "SdQ_19h9w70K",
    "outputId": "f286025e-0f36-420a-c967-327415c4fd60"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import chars2vec \n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras import datasets, layers, models, losses, callbacks, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PWm1tEArY6gG"
   },
   "outputs": [],
   "source": [
    "def f1(predictions, gold):\n",
    "    \"\"\"\n",
    "    Calculates F1 score(a.k.a. DICE)\n",
    "    \n",
    "    Args:\n",
    "        predictions: a list of predicted offsets\n",
    "        gold: a list of offsets serving as the ground truth\n",
    "        \n",
    "    Returns: \n",
    "        a float score between 0 and 1\n",
    "    \"\"\"\n",
    "    if len(gold) == 0:\n",
    "        return 1 if len(predictions) == 0 else 0\n",
    "    if len(predictions) == 0:\n",
    "        return 0\n",
    "    predictions_set = set(predictions)\n",
    "    gold_set = set(gold)\n",
    "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
    "    denom = len(predictions_set) + len(gold_set)\n",
    "    return float(nom)/float(denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to extract text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PKEOlxEWy9dz"
   },
   "outputs": [],
   "source": [
    "def read_text_data(filename):\n",
    "    \"\"\"\n",
    "    Reads a csv file to extract text.\n",
    "    \n",
    "    Args:\n",
    "        filename: a string specifying the filename / path\n",
    "        \n",
    "    Returns:\n",
    "        A list of text sentences\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            data.append(row['text'])\n",
    "    csvfile.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to extract spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DI0YNeXszIAW"
   },
   "outputs": [],
   "source": [
    "def read_data_span(filename):\n",
    "    \"\"\"\n",
    "    Reads a csv file to extract the toxic spans.\n",
    "    \n",
    "    Args:\n",
    "        filename: a string specifying the filename / path\n",
    "        \n",
    "    Returns:\n",
    "        data: a list of strings of the toxic chars, \n",
    "        will look like '[1,2,3]' so it'll have to be split\n",
    "        \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            data.append(row['span'])\n",
    "    csvfile.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40577,
     "status": "ok",
     "timestamp": 1614114817551,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "SJyrue9gzNtg",
    "outputId": "b13d3049-756b-4544-c96a-0c0bbb673d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths equal: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = read_text_data('../data/tsd_train_readable.csv')\n",
    "spans = read_data_span('../data/tsd_train_readable.csv')\n",
    "texts.extend(read_text_data('../data/tsd_trial_readable.csv'))\n",
    "spans.extend(read_data_span('../data/tsd_trial_readable.csv'))\n",
    "\n",
    "\n",
    "processed_texts = []\n",
    "processed_spans = []\n",
    "print(f\"Lengths equal: {len(texts)==len(spans)}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the maximum comment size (in no. of chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sgC4OBpIzWLF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max size of sentence (in chars): 1000\n"
     ]
    }
   ],
   "source": [
    "max_size = 0\n",
    "for i in range(0, len(texts)-1):\n",
    "    if len(texts[i]) > max_size:\n",
    "        max_size = len(texts[i])\n",
    "print(f\"max size of sentence (in chars): {max_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the training data, after analysis the max sentence size is 1000 characters long, also removing empty strings and split the spans in to actual lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BPLticdKzTQ3"
   },
   "outputs": [],
   "source": [
    "c2v_model = chars2vec.load_model('eng_50')\n",
    "word_limit = 1024\n",
    "for i in range(0, len(texts)-1):\n",
    "    to_use = True\n",
    "    if len(texts[i]) > word_limit:\n",
    "        to_use = False\n",
    "    if texts[i] == \"\":\n",
    "        to_use = False\n",
    "    new_spans = [int(j) for j in spans[i][1:-1].split(\", \")]\n",
    "    if max(new_spans) > len(texts[i]) - 1:\n",
    "        to_use = False\n",
    "    if to_use:\n",
    "        if spans[i] != []:\n",
    "            full_span = [[0,0,1] for j in range(0, word_limit)]\n",
    "            for char_offset in new_spans:\n",
    "                full_span[char_offset] = [1,0,0]\n",
    "            for j in range(0, len(texts[i])-1):\n",
    "                if full_span[j][1] == 0 and full_span[j][2] == 1:\n",
    "                    full_span[j] = [0,1,0]\n",
    "        else:\n",
    "            full_span = [[1,0,0] for j in range(0, len(texts[i]))]           \n",
    "        processed_texts.append(texts[i])\n",
    "        processed_spans.append(full_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training and testing datasets with numpy zero arrays, this is to allow us to pad the end\n",
    "Of the toxic span with zeros as it is a fully convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7ErQHD8LzY1Z"
   },
   "outputs": [],
   "source": [
    "train_Y = np.zeros(shape=(len(processed_spans), 1024, 3))\n",
    "train_X = np.zeros(shape=(len(processed_texts), 1024, 50))         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terrible Python best practise but you might wanna manually free up some memory. This is going to be a very large compuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0nexzfqHyxC9"
   },
   "outputs": [],
   "source": [
    "del texts\n",
    "del spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Mlvr4eF9zfTl"
   },
   "outputs": [],
   "source": [
    "for x, string in enumerate(processed_texts):\n",
    "    for y, char in enumerate(string):\n",
    "            char_vect = c2v_model.vectorize_words([char])\n",
    "            train_X[x][y] = [word_vect for word_vect in char_vect[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c61dItQbzljv"
   },
   "outputs": [],
   "source": [
    "for x, label in enumerate(processed_spans):\n",
    "    for y, output in enumerate(label):\n",
    "        train_Y[x][y] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train_X and train_Y into training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9U660qMLzz8F"
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the shape of the train and val datasets, should be ([sample_size], 1000, 50) and ([sample_size], 1000, 3) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7102, 1024, 50)\n",
      "(7102, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a global variable would be out of scope for the callback object class manually create a HighScore class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighScore:\n",
    "    def __init__(self):\n",
    "        self.high_score = 0\n",
    "    def get_high_score(self):\n",
    "        return self.high_score\n",
    "    def set_high_score(self, new_score):\n",
    "        self.high_score = new_score\n",
    "high_score = HighScore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free up more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mbvc3atVucHH"
   },
   "outputs": [],
   "source": [
    "del processed_texts\n",
    "del processed_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tecSwLEfFTez"
   },
   "source": [
    "A prediction callback to act as a validation step, as the tensor is of a different shape to the F1 score of SemEval we must Convert it into it's proper form before checking the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Lh9DVvyz6NjV"
   },
   "outputs": [],
   "source": [
    "class PredictionCallback(callbacks.Callback):    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(val_X)\n",
    "        scores = []\n",
    "        for x, pred in enumerate(y_pred):\n",
    "            score = f1([j for j, i in enumerate(pred) if np.argmax(i) == 0], [j for j, i in enumerate(val_Y[x]) if np.argmax(i) == 0])\n",
    "            scores.append(score)\n",
    "        score = statistics.mean(scores)\n",
    "        if score > high_score.get_high_score():\n",
    "            high_score.set_high_score(score)\n",
    "            model.save(f\"DeconvNet_deep_checkpoint_sko\")\n",
    "        print(f\"F1 score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SegNet(is_deep=False, is_sko=False):\n",
    "    \n",
    "    if is_sko:\n",
    "        final_output_layer = 1\n",
    "    else:\n",
    "        final_output_layer = 9\n",
    "        \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape = train_X.shape[1:]))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv1D(filters=256, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    if is_deep:\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        model.add(layers.Conv1D(filters=512, kernel_size=9, strides=1, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.ELU())\n",
    "        model.add(layers.MaxPooling1D(strides=2))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        model.add(layers.UpSampling1D(size=2))\n",
    "        model.add(layers.Conv1D(filters=512, kernel_size=9, strides=1, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.ELU())\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1D(filters=256, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "\n",
    "    model.add(layers.Conv1D(filters=3, kernel_size=final_output_layer, strides=1, padding='same', activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def get_DeconvNet(is_deep=False, is_sko=False):\n",
    "    \n",
    "    if is_sko:\n",
    "        final_output_layer = 1\n",
    "    else:\n",
    "        final_output_layer = 9\n",
    "        \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape = train_X.shape[1:]))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv1D(filters=256, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.MaxPooling1D(strides=2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    if is_deep:\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        model.add(layers.Conv1D(filters=512, kernel_size=9, strides=1, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.ELU())\n",
    "        model.add(layers.MaxPooling1D(strides=2))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        model.add(layers.UpSampling1D(size=2))\n",
    "        model.add(layers.Conv1DTranspose(filters=512, kernel_size=9, strides=1, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.ELU())\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1DTranspose(filters=256, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1DTranspose(filters=128, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1DTranspose(filters=64, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "\n",
    "    model.add(layers.UpSampling1D(size=2))\n",
    "    model.add(layers.Conv1DTranspose(filters=32, kernel_size=9, strides=1, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ELU())\n",
    "\n",
    "    model.add(layers.Conv1D(filters=3, kernel_size=final_output_layer, strides=1, padding='same', activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def get_UNet():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model and view it's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 13759843,
     "status": "error",
     "timestamp": 1614130462871,
     "user": {
      "displayName": "James Bedwell",
      "photoUrl": "",
      "userId": "08359185221310017519"
     },
     "user_tz": 0
    },
    "id": "H3YXwcUTzw6X",
    "outputId": "0e155ac1-fd98-431b-9b04-b9e983012148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 1024, 32)          14432     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024, 32)          128       \n",
      "_________________________________________________________________\n",
      "elu (ELU)                    (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 512, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 512, 64)           18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512, 64)           256       \n",
      "_________________________________________________________________\n",
      "elu_1 (ELU)                  (None, 512, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 256, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 256, 128)          73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256, 128)          512       \n",
      "_________________________________________________________________\n",
      "elu_2 (ELU)                  (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 128, 256)          295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128, 256)          1024      \n",
      "_________________________________________________________________\n",
      "elu_3 (ELU)                  (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 64, 512)           1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64, 512)           2048      \n",
      "_________________________________________________________________\n",
      "elu_4 (ELU)                  (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 32, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32, 512)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d (UpSampling1D) (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose (Conv1DTran (None, 64, 512)           2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64, 512)           2048      \n",
      "_________________________________________________________________\n",
      "elu_5 (ELU)                  (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1 (None, 128, 512)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_1 (Conv1DTr (None, 128, 256)          1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128, 256)          1024      \n",
      "_________________________________________________________________\n",
      "elu_6 (ELU)                  (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_2 (UpSampling1 (None, 256, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_2 (Conv1DTr (None, 256, 128)          295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256, 128)          512       \n",
      "_________________________________________________________________\n",
      "elu_7 (ELU)                  (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_3 (UpSampling1 (None, 512, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_3 (Conv1DTr (None, 512, 64)           73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512, 64)           256       \n",
      "_________________________________________________________________\n",
      "elu_8 (ELU)                  (None, 512, 64)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_4 (UpSampling1 (None, 1024, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_transpose_4 (Conv1DTr (None, 1024, 32)          18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 1024, 32)          128       \n",
      "_________________________________________________________________\n",
      "elu_9 (ELU)                  (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1024, 3)           99        \n",
      "=================================================================\n",
      "Total params: 5,517,155\n",
      "Trainable params: 5,513,187\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_DeconvNet(is_deep=True, is_sko=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "222/222 [==============================] - 51s 183ms/step - loss: 0.1623 - accuracy: 0.9549\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.061539070897734395\n",
      "Epoch 2/300\n",
      "222/222 [==============================] - 40s 180ms/step - loss: 0.0629 - accuracy: 0.9812\n",
      "F1 score: 0.02921977791292851\n",
      "Epoch 3/300\n",
      "222/222 [==============================] - 40s 181ms/step - loss: 0.0623 - accuracy: 0.9807\n",
      "F1 score: 0.023460417407562852\n",
      "Epoch 4/300\n",
      "222/222 [==============================] - 41s 187ms/step - loss: 0.0582 - accuracy: 0.9820\n",
      "F1 score: 0.03824460483594401\n",
      "Epoch 5/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0592 - accuracy: 0.9816\n",
      "F1 score: 0.008196094563516282\n",
      "Epoch 6/300\n",
      "222/222 [==============================] - 42s 190ms/step - loss: 0.0572 - accuracy: 0.9823\n",
      "F1 score: 0.030128104396471095\n",
      "Epoch 7/300\n",
      "222/222 [==============================] - 43s 191ms/step - loss: 0.0581 - accuracy: 0.9816\n",
      "F1 score: 0\n",
      "Epoch 8/300\n",
      "222/222 [==============================] - 41s 186ms/step - loss: 0.0572 - accuracy: 0.9823\n",
      "F1 score: 0.0421214587454982\n",
      "Epoch 9/300\n",
      "222/222 [==============================] - 41s 186ms/step - loss: 0.0554 - accuracy: 0.9828\n",
      "F1 score: 0.04465571767272773\n",
      "Epoch 10/300\n",
      "222/222 [==============================] - 41s 186ms/step - loss: 0.0568 - accuracy: 0.9825\n",
      "F1 score: 0.05667767437486048\n",
      "Epoch 11/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0559 - accuracy: 0.9824\n",
      "F1 score: 0.0526660185025752\n",
      "Epoch 12/300\n",
      "222/222 [==============================] - 42s 189ms/step - loss: 0.0544 - accuracy: 0.9829\n",
      "F1 score: 0.027352740225173096\n",
      "Epoch 13/300\n",
      "222/222 [==============================] - 42s 187ms/step - loss: 0.0563 - accuracy: 0.9822\n",
      "F1 score: 0.05657851238715397\n",
      "Epoch 14/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0572 - accuracy: 0.9815\n",
      "F1 score: 0.046384608900404016\n",
      "Epoch 15/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0552 - accuracy: 0.9819\n",
      "F1 score: 0.035528665612371106\n",
      "Epoch 16/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0538 - accuracy: 0.9824\n",
      "F1 score: 0.047640868358453305\n",
      "Epoch 17/300\n",
      "222/222 [==============================] - 42s 189ms/step - loss: 0.0531 - accuracy: 0.9823\n",
      "F1 score: 0.043983607377942185\n",
      "Epoch 18/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0569 - accuracy: 0.9803\n",
      "F1 score: 0\n",
      "Epoch 19/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0531 - accuracy: 0.9816\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.06740672799131527\n",
      "Epoch 20/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0524 - accuracy: 0.9821\n",
      "F1 score: 0.05167782413532231\n",
      "Epoch 21/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0502 - accuracy: 0.9830\n",
      "F1 score: 0.027919836791781498\n",
      "Epoch 22/300\n",
      "222/222 [==============================] - 41s 187ms/step - loss: 0.0511 - accuracy: 0.9830s - loss: 0 - ETA: 0s - loss: 0.0511 - accuracy: 0.98\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.08306883012874727\n",
      "Epoch 23/300\n",
      "222/222 [==============================] - 42s 189ms/step - loss: 0.0488 - accuracy: 0.9834\n",
      "F1 score: 0.039019948716674344\n",
      "Epoch 24/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0479 - accuracy: 0.9838\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.08414825363861125\n",
      "Epoch 25/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0472 - accuracy: 0.9840\n",
      "F1 score: 0.05459213931661974\n",
      "Epoch 26/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0452 - accuracy: 0.9846\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.12308993117347186\n",
      "Epoch 27/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0453 - accuracy: 0.9846s - loss: 0.0453 - \n",
      "F1 score: 0.03442957274814468\n",
      "Epoch 28/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0450 - accuracy: 0.9843\n",
      "F1 score: 0.029251277183146133\n",
      "Epoch 29/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0432 - accuracy: 0.9850\n",
      "F1 score: 0.07659261048121316\n",
      "Epoch 30/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0429 - accuracy: 0.9850s - loss: 0 - ETA: 2s -\n",
      "F1 score: 0.08930862766808695\n",
      "Epoch 31/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0421 - accuracy: 0.9852\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.1489954863549017\n",
      "Epoch 32/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0415 - accuracy: 0.9853\n",
      "F1 score: 0.05029441684318825\n",
      "Epoch 33/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0420 - accuracy: 0.9851\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.2560872043420666\n",
      "Epoch 34/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0403 - accuracy: 0.9858\n",
      "F1 score: 0.12471407310375479\n",
      "Epoch 35/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0400 - accuracy: 0.9858\n",
      "F1 score: 0.10306210035363014\n",
      "Epoch 36/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0391 - accuracy: 0.9859s - loss: 0.0391 - ac\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.2635104710370171\n",
      "Epoch 37/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0386 - accuracy: 0.9863\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.30158369080946307\n",
      "Epoch 38/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0382 - accuracy: 0.9864\n",
      "F1 score: 0.26081645962690947\n",
      "Epoch 39/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0368 - accuracy: 0.9868\n",
      "F1 score: 0.1903326838434198\n",
      "Epoch 40/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0365 - accuracy: 0.9869\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.3618086486291279\n",
      "Epoch 41/300\n",
      "222/222 [==============================] - 41s 186ms/step - loss: 0.0353 - accuracy: 0.9873\n",
      "F1 score: 0.3136042878163785\n",
      "Epoch 42/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0362 - accuracy: 0.9869\n",
      "F1 score: 0.3031253224208115\n",
      "Epoch 43/300\n",
      "222/222 [==============================] - 42s 188ms/step - loss: 0.0355 - accuracy: 0.9871\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.3783506820787405\n",
      "Epoch 44/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0350 - accuracy: 0.9873\n",
      "F1 score: 0.3021749416470505\n",
      "Epoch 45/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0342 - accuracy: 0.9876\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.38929389587471425\n",
      "Epoch 46/300\n",
      "222/222 [==============================] - 41s 186ms/step - loss: 0.0348 - accuracy: 0.9873\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.39057984235833815\n",
      "Epoch 47/300\n",
      "222/222 [==============================] - 41s 186ms/step - loss: 0.0335 - accuracy: 0.9879\n",
      "F1 score: 0.3503071023958344\n",
      "Epoch 48/300\n",
      "222/222 [==============================] - 41s 186ms/step - loss: 0.0333 - accuracy: 0.9879\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.40320762772689756\n",
      "Epoch 49/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0331 - accuracy: 0.9879\n",
      "F1 score: 0.32514991705323976\n",
      "Epoch 50/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0331 - accuracy: 0.9880\n",
      "F1 score: 0.40012142074376067\n",
      "Epoch 51/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0326 - accuracy: 0.9879\n",
      "F1 score: 0.3516454287901114\n",
      "Epoch 52/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0314 - accuracy: 0.9885\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.4094686562359182\n",
      "Epoch 53/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0314 - accuracy: 0.9885\n",
      "F1 score: 0.38405647830611106\n",
      "Epoch 54/300\n",
      "222/222 [==============================] - 41s 184ms/step - loss: 0.0334 - accuracy: 0.9879\n",
      "F1 score: 0.39104233623789814\n",
      "Epoch 55/300\n",
      "222/222 [==============================] - 41s 185ms/step - loss: 0.0312 - accuracy: 0.9885\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.4441943893799052\n",
      "Epoch 56/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0306 - accuracy: 0.9887\n",
      "F1 score: 0.413401473831393\n",
      "Epoch 57/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0308 - accuracy: 0.9886\n",
      "F1 score: 0.4377243569452052\n",
      "Epoch 58/300\n",
      "222/222 [==============================] - 19s 85ms/step - loss: 0.0296 - accuracy: 0.9892\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.46129262323799636\n",
      "Epoch 59/300\n",
      "222/222 [==============================] - 19s 87ms/step - loss: 0.0303 - accuracy: 0.9888\n",
      "F1 score: 0.412519245043192\n",
      "Epoch 60/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0300 - accuracy: 0.9888\n",
      "F1 score: 0.4358195318541998\n",
      "Epoch 61/300\n",
      "222/222 [==============================] - 19s 87ms/step - loss: 0.0295 - accuracy: 0.9891\n",
      "F1 score: 0.4181769024366487\n",
      "Epoch 62/300\n",
      "222/222 [==============================] - 20s 88ms/step - loss: 0.0284 - accuracy: 0.98941s - l\n",
      "F1 score: 0.4274079849922216\n",
      "Epoch 63/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0285 - accuracy: 0.9894\n",
      "F1 score: 0.4422520881533393\n",
      "Epoch 64/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0285 - accuracy: 0.9895\n",
      "F1 score: 0.4397267514989694\n",
      "Epoch 65/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0283 - accuracy: 0.9895\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.462162476575771\n",
      "Epoch 66/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0279 - accuracy: 0.9894\n",
      "F1 score: 0.4290063530801244\n",
      "Epoch 67/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0284 - accuracy: 0.9893\n",
      "F1 score: 0.44688581787575965\n",
      "Epoch 68/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0282 - accuracy: 0.9894\n",
      "F1 score: 0.4031103925040649\n",
      "Epoch 69/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0275 - accuracy: 0.9898\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.4636886909877888\n",
      "Epoch 70/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0273 - accuracy: 0.9897\n",
      "F1 score: 0.4582134052899824\n",
      "Epoch 71/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0264 - accuracy: 0.9900\n",
      "F1 score: 0.46195996354170943\n",
      "Epoch 72/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0269 - accuracy: 0.9899\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.4659858273190834\n",
      "Epoch 73/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0261 - accuracy: 0.9902\n",
      "F1 score: 0.44111900073699434\n",
      "Epoch 74/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0262 - accuracy: 0.9901\n",
      "F1 score: 0.4443493239063236\n",
      "Epoch 75/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0269 - accuracy: 0.98980s - loss: 0.0269 - accuracy\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.4670154859713643\n",
      "Epoch 76/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0259 - accuracy: 0.9902\n",
      "F1 score: 0.3777885946781156\n",
      "Epoch 77/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0312 - accuracy: 0.9883\n",
      "F1 score: 0.4629171767728282\n",
      "Epoch 78/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0274 - accuracy: 0.9898\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.4711424370068098\n",
      "Epoch 79/300\n",
      "222/222 [==============================] - 19s 87ms/step - loss: 0.0264 - accuracy: 0.9900\n",
      "F1 score: 0.43734233292534436\n",
      "Epoch 80/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0255 - accuracy: 0.9903\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.4785055027754193\n",
      "Epoch 81/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0257 - accuracy: 0.9903\n",
      "F1 score: 0.47108353160803335\n",
      "Epoch 82/300\n",
      "222/222 [==============================] - 19s 85ms/step - loss: 0.0247 - accuracy: 0.9906\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.49576397794850086\n",
      "Epoch 83/300\n",
      "222/222 [==============================] - 19s 85ms/step - loss: 0.0252 - accuracy: 0.9905\n",
      "F1 score: 0.4793337830088146\n",
      "Epoch 84/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0253 - accuracy: 0.9904\n",
      "F1 score: 0.4814307672824871\n",
      "Epoch 85/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0247 - accuracy: 0.9906\n",
      "F1 score: 0.4724748895234356\n",
      "Epoch 86/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0241 - accuracy: 0.9908\n",
      "F1 score: 0.48182094364617234\n",
      "Epoch 87/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0252 - accuracy: 0.9904\n",
      "F1 score: 0.48669023630276464\n",
      "Epoch 88/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0249 - accuracy: 0.9906\n",
      "F1 score: 0.4837182023229113\n",
      "Epoch 89/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0246 - accuracy: 0.9907\n",
      "F1 score: 0.4748146597566966\n",
      "Epoch 90/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0241 - accuracy: 0.9909\n",
      "F1 score: 0.4709421224428818\n",
      "Epoch 91/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0241 - accuracy: 0.9908\n",
      "F1 score: 0.48441017754322135\n",
      "Epoch 92/300\n",
      "222/222 [==============================] - 19s 85ms/step - loss: 0.0240 - accuracy: 0.9909\n",
      "F1 score: 0.46975895322821676\n",
      "Epoch 93/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0232 - accuracy: 0.9912\n",
      "F1 score: 0.466864101226932\n",
      "Epoch 94/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0246 - accuracy: 0.9906\n",
      "F1 score: 0.47066757828858347\n",
      "Epoch 95/300\n",
      "222/222 [==============================] - 19s 85ms/step - loss: 0.0232 - accuracy: 0.9912\n",
      "F1 score: 0.47154478941102745\n",
      "Epoch 96/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0234 - accuracy: 0.9911\n",
      "F1 score: 0.4722264163021603\n",
      "Epoch 97/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0234 - accuracy: 0.9911\n",
      "F1 score: 0.45619831727108173\n",
      "Epoch 98/300\n",
      "222/222 [==============================] - 19s 85ms/step - loss: 0.0233 - accuracy: 0.9911\n",
      "F1 score: 0.45636079203967805\n",
      "Epoch 99/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0240 - accuracy: 0.9910\n",
      "F1 score: 0.47473993765995376\n",
      "Epoch 100/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0238 - accuracy: 0.9910\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.49911370408902506\n",
      "Epoch 101/300\n",
      "222/222 [==============================] - 19s 87ms/step - loss: 0.0226 - accuracy: 0.9914\n",
      "F1 score: 0.4749008670111446\n",
      "Epoch 102/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0221 - accuracy: 0.9917\n",
      "F1 score: 0.49871582574052287\n",
      "Epoch 103/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0227 - accuracy: 0.9913\n",
      "F1 score: 0.47980060764544\n",
      "Epoch 104/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0225 - accuracy: 0.99141s - loss: 0.0225 - accuracy: 0. - ETA: 1s - loss: - ETA: 0s - loss: 0.0225 - accuracy\n",
      "F1 score: 0.4987913285437125\n",
      "Epoch 105/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0224 - accuracy: 0.9915\n",
      "F1 score: 0.4553633435588382\n",
      "Epoch 106/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0221 - accuracy: 0.9916\n",
      "F1 score: 0.48230099652349034\n",
      "Epoch 107/300\n",
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0224 - accuracy: 0.9914\n",
      "F1 score: 0.48815301478454726\n",
      "Epoch 108/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/222 [==============================] - 19s 86ms/step - loss: 0.0215 - accuracy: 0.9918\n",
      "F1 score: 0.49895076925286996\n",
      "Epoch 109/300\n",
      "222/222 [==============================] - 19s 87ms/step - loss: 0.0237 - accuracy: 0.9910\n",
      "INFO:tensorflow:Assets written to: DeconvNet_deep_checkpoint_sko\\assets\n",
      "F1 score: 0.5042509552421988\n",
      "Epoch 110/300\n",
      " 99/222 [============>.................] - ETA: 10s - loss: 0.0212 - accuracy: 0.9919"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, train_Y, epochs=300, batch_size=32, callbacks=[PredictionCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model and check final validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rACAu8nCZ01L"
   },
   "outputs": [],
   "source": [
    "model.save(f\"DeconvNet_deep_model_300_epochs_sko\")\n",
    "scores = []\n",
    "pred_Y = model.predict(val_X)\n",
    "\n",
    "for x, pred in enumerate(pred_Y):\n",
    "    y_pred_f1_compatible = [j for j, i in enumerate(pred) if np.argmax(i) == 0]\n",
    "    y_true_f1_compatible = [j for j, i in enumerate(val_Y[x]) if np.argmax(i) == 0]\n",
    "    if val_Y[x] == []:\n",
    "        y_pred_f1_compatible = []\n",
    "    score = f1(y_pred_f1_compatible, y_true_f1_compatible)\n",
    "    scores.append(score)\n",
    "\n",
    "print('avg F1 %g' % statistics.mean(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually view some predictions to check validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SgYIaccv1Mj"
   },
   "outputs": [],
   "source": [
    "for x, pred in enumerate(pred_Y):\n",
    "    score = f1([j for j, i in enumerate(pred) if np.argmax(i) == 0], [j for j, i in enumerate(val_Y[x]) if np.argmax(i) == 0])\n",
    "    print(f\"F1 score: {score}\")\n",
    "    print(f\"Predicted span one_hot: {[np.argmax(i) for i in pred]}\")\n",
    "    print(f\"Predicted span: {[j for j, i in enumerate(pred) if np.argmax(i) == 0]}\")\n",
    "    print(f\"Ground truth span: {[j for j, i in enumerate(val_Y[x]) if np.argmax(i) == 0]}\" + \"\\n\")\n",
    "    if x == 100:\n",
    "          break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP79GJAw0365QrEWLKMJifk",
   "collapsed_sections": [],
   "name": "Toxic_Text_Segmentatio_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
